{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "import time\n",
    "\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scrape Code"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Page Scrape Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_page_data(table_rows):\n",
    "\n",
    "    # Extract team names and odds\n",
    "    game_dates = []\n",
    "    home_teams = []\n",
    "    away_teams = []\n",
    "    home_odds = []\n",
    "    away_odds = []\n",
    "    home_scores = []\n",
    "    away_scores = []\n",
    "\n",
    "    for i, table_row in enumerate(table_rows):\n",
    "\n",
    "        ## Get the date\n",
    "        game_date = table_row.find('div', class_='text-black-main font-main w-full truncate text-xs font-normal leading-5')\n",
    "        try:\n",
    "            game_date = game_date.get_text(strip=True)\n",
    "        except:\n",
    "            game_date = last_game_date\n",
    "\n",
    "        last_game_date = game_date\n",
    "        game_dates.append(game_date)\n",
    "\n",
    "        ## Get the team names\n",
    "        team_names = table_row.find_all('p', class_='truncate participant-name')\n",
    "        home_teams.append( team_names[0].get_text(strip=True) )\n",
    "        away_teams.append( team_names[1].get_text(strip=True) )\n",
    "\n",
    "        ## Get the scores\n",
    "        team_scores = table_row.find_all('div', class_='min-mt:!flex')\n",
    "        if len(team_scores) != 0:\n",
    "            home_scores.append( team_scores[0].get_text(strip=True) )\n",
    "            away_scores.append( team_scores[1].get_text(strip=True) )\n",
    "        else:\n",
    "            home_scores.append(np.nan)\n",
    "            away_scores.append(np.nan)\n",
    "\n",
    "        ## Get the odds\n",
    "        odds = table_row.find_all('p', class_='height-content')\n",
    "        home_odds.append( odds[0].get_text(strip=True) )\n",
    "        away_odds.append( odds[1].get_text(strip=True) ) \n",
    "\n",
    "\n",
    "    odds_df = pd.DataFrame({\n",
    "        'Date': game_dates,\n",
    "        'Home': home_teams,\n",
    "        'Away': away_teams,\n",
    "        'Home_Odds': home_odds,\n",
    "        'Away_Odds': away_odds,\n",
    "        'Home_Score': home_scores,\n",
    "        'Away_Score': away_scores\n",
    "    })\n",
    "\n",
    "    return odds_df\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scrape Page and Create Dataframe Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_html_get_data(year, page_num):\n",
    "    oddsportal_link = fr'https://www.oddsportal.com/baseball/usa/mlb-{year}/results/#/page/{page_num}'\n",
    "\n",
    "    scrolls = 2\n",
    "\n",
    "    # Initialize the WebDriver (you should specify the path to your WebDriver executable)\n",
    "    driver = webdriver.Chrome()\n",
    "\n",
    "    # Load the webpage\n",
    "    driver.get(oddsportal_link)\n",
    "\n",
    "    # Define a function to scroll down the page\n",
    "    def scroll_down(driver):\n",
    "        driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "        time.sleep(1)  # Adjust the sleep time as needed\n",
    "\n",
    "    # Scroll down a few times to load more content\n",
    "    for _ in range(scrolls):  # You can adjust the number of times to scroll\n",
    "        scroll_down(driver)\n",
    "\n",
    "    # Get the page source (which includes dynamically loaded content)\n",
    "    page_source = driver.page_source\n",
    "\n",
    "    # Close the WebDriver\n",
    "    driver.quit()\n",
    "\n",
    "    soup = BeautifulSoup(page_source, 'html.parser')\n",
    "\n",
    "    table_rows = soup.find_all('div', class_='eventRow')\n",
    "    try:\n",
    "        odds_df = get_page_data(table_rows)\n",
    "    except IndexError:\n",
    "        print(f\"Error in {year} on page {page_num}\")\n",
    "        odds_df = pd.DataFrame({})\n",
    "\n",
    "    return odds_df\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Scrape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "scrapeBettingOdds = False\n",
    "\n",
    "year_range = range(2023, 2024)\n",
    "page_seq = range(1, 53)\n",
    "\n",
    "if scrapeBettingOdds:\n",
    "    for year in tqdm(year_range):\n",
    "        odds_dfs = []\n",
    "        for page in page_seq:\n",
    "            odds_dfs.append( scrape_html_get_data(year, page) )\n",
    "\n",
    "        year_odds = pd.concat(odds_dfs)\n",
    "        year_odds.to_csv(fr'/Users/willmiraglia/Desktop/Datathon 2024/Odds/{year}_Odds.csv', index=False)\n",
    "        "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Odds Conversion Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def american_to_implied(odds):\n",
    "    if odds >= 0:\n",
    "        return 100*(100 / (odds + 100))\n",
    "    else:\n",
    "        return 100*(abs(odds) / (abs(odds) + 100))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download and Clean Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "odds_dict = {}\n",
    "for year in range(2008, 2024):\n",
    "    baseball_base_path = fr'/Users/willmiraglia/Desktop/Datathon 2024/Odds/{year}_Odds.csv'\n",
    "    odds_df = pd.read_csv(baseball_base_path)\n",
    "    odds_dict[year] = odds_df\n",
    "\n",
    "all_odds_df = pd.concat(odds_dict)\n",
    "all_odds_df = all_odds_df.reset_index(drop=True)\n",
    "all_odds_df['Date'] = all_odds_df['Date'].apply(lambda x: pd.to_datetime(' '.join(x.split()[:3])).date())\n",
    "all_odds_df['Home_Odds'] = all_odds_df['Home_Odds'].apply(lambda x: float(x) if x != '-' else None)\n",
    "all_odds_df['Away_Odds'] = all_odds_df['Away_Odds'].apply(lambda x: float(x) if x != '-' else None)\n",
    "all_odds_df = all_odds_df.sort_values(by='Date', ascending=True).dropna().reset_index(drop=True)\n",
    "\n",
    "excluded_teams = ['Campeche', 'National League', 'American League', 'Australia', 'Sacramento River Cats', 'Tabasco', 'Northeastern']\n",
    "all_odds_df = all_odds_df[~all_odds_df['Home'].isin(excluded_teams) & ~all_odds_df['Away'].isin(excluded_teams)]\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert Team Names to Abbreviations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "team_name_to_abrev = {\n",
    "    'Oakland Athletics':     'OAK', \n",
    "    'Washington Nationals':  'WAS', \n",
    "    'Chicago Cubs':          'CHC',\n",
    "    'Philadelphia Phillies': 'PHI', \n",
    "    'Cleveland Guardians':   'CLE',\n",
    "    'Baltimore Orioles':     'BAL', \n",
    "    'Miami Marlins':         'MIA', \n",
    "    'Seattle Mariners':      'SEA',\n",
    "    'Atlanta Braves':        'ATL', \n",
    "    'Cincinnati Reds':       'CIN', \n",
    "    'Detroit Tigers':        'DET',\n",
    "    'Los Angeles Dodgers':   'LAD', \n",
    "    'St.Louis Cardinals':    'STL', \n",
    "    'Minnesota Twins':       'MIN',\n",
    "    'New York Yankees':      'NYY', \n",
    "    'San Diego Padres':      'SD', \n",
    "    'Arizona Diamondbacks':  'AZ',\n",
    "    'Colorado Rockies':      'COL', \n",
    "    'Houston Astros':        'HOU', \n",
    "    'Chicago White Sox':     'CWS',\n",
    "    'San Francisco Giants':  'SF', \n",
    "    'Pittsburgh Pirates':    'PIT', \n",
    "    'Los Angeles Angels':    'LAA',\n",
    "    'Tampa Bay Rays':        'TB',\n",
    "    'New York Mets':         'NYM', \n",
    "    'Toronto Blue Jays':     'TOR',\n",
    "    'Kansas City Royals':    'KC', \n",
    "    'Boston Red Sox':        'BOS', \n",
    "    'Texas Rangers':         'TEX',\n",
    "    'Milwaukee Brewers':     'MIL' \n",
    "}\n",
    "\n",
    "all_odds_df['Home'] = all_odds_df['Home'].map(team_name_to_abrev)\n",
    "all_odds_df['Away'] = all_odds_df['Away'].map(team_name_to_abrev)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert Odds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_odds_df['Home_IP'] = all_odds_df['Home_Odds'].apply(american_to_implied)\n",
    "all_odds_df['Away_IP'] = all_odds_df['Away_Odds'].apply(american_to_implied)\n",
    "\n",
    "home_ip = all_odds_df['Home_IP']\n",
    "away_ip = all_odds_df['Away_IP']\n",
    "total_ip = all_odds_df['Home_IP'] + all_odds_df['Away_IP']\n",
    "\n",
    "all_odds_df['Home_IP'] = home_ip / total_ip\n",
    "all_odds_df['Away_IP'] = away_ip / total_ip\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download Final DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_ip_df = all_odds_df[['Date', 'Home', 'Away', 'Home_Score', 'Away_Score', 'Home_IP', 'Away_IP']]\n",
    "all_ip_df = all_ip_df.rename(columns={'Date': 'game_date', \n",
    "                                      'Home': 'home_team', \n",
    "                                      'Away': 'away_team', \n",
    "                                      'Home_Score': 'home_score',\n",
    "                                      'Away_Score': 'away_score',\n",
    "                                      'Home_IP': 'home_odds', \n",
    "                                      'Away_IP': 'away_odds'\n",
    "                                    })\n",
    "\n",
    "all_ip_df.to_csv(r'/Users/willmiraglia/Desktop/Datathon 2024/Odds/Full_Odds.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chatgpt test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
